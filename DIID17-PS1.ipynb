{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 1, due March 1 by midnight\n",
    "\n",
    "The goal of this problem set is to replicate and extend the results of  Jean et al.'s 2016 paper, \"Combining satellite imagery and machine learning to predict poverty.\" This problem set will be challenging and time-consuming, so I suggest you start immediately. Your first step should be to carefully read <a href=\"https://pdfs.semanticscholar.org/1b3a/c4b4187a3dbc9373869e7774b1dc63f748d2.pdf\">the original paper</a>  as well as the <a href=\"http://science.sciencemag.org/content/sci/suppl/2016/08/19/353.6301.790.DC1/Jean.SM.pdf\">supplementary materials</a>.\n",
    "\n",
    "For this assignment, we will focus on the country of Rwanda. You will need to download three distinct datasets, including DHS data, satellite data from the Google Maps API, as well as nighttime luminosity data. The DHS data requires registration (which can take several days to be approved), and the Google Maps API is rate-limited, so it will necessarily take you several days to download the requisite data, so make sure to **get started on those steps asap**. The deep learning section may also take several hours to compute (or days, if you have a slow computer), so don't save it until the last minute.\n",
    "\n",
    "## Overview of the problem set\n",
    "\n",
    "These are the key steps in the problem set:\n",
    "\n",
    "1. Download satellite night lights images from NOAA\n",
    "2. Download DHS data for Rwanda\n",
    "3. Test whether night lights data can predict wealth, as observed in DHS\n",
    "4. Download daytime satellite imagery from Google Maps\n",
    "5. Test whether basic features of daytime imagery can predict wealth\n",
    "6. Extract features from daytime imagery using deep learning libraries\n",
    "7. Replicate final model and results of Jean et al (2016)\n",
    "8. Construct maps showing the predicted distribution of wealth in Rwanda\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download nightlights for Rwanda\n",
    "\n",
    "- **INPUT**:\n",
    " - None\n",
    "- **OUTPUT**: \n",
    " - `F182010.v4d_web.stable_lights.avg_vis.tif`: Single image file giving nightlights intensity around the world\n",
    "\n",
    "Go to the [DMSP-OLS website](https://ngdc.noaa.gov/eog/dmsp/downloadV4composites.html) and download the satellite nighttime luminosity data (roughly 400MB). We will use the one from 2010. The archive they provide constains several files. Feel free to explore these files. We will only be using the file F182010.v4d_web.stable_lights.avg_vis.tif.\n",
    "\n",
    "A code snippet to get you started is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'wget'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7ef91036da83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mwget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnight_image_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://ngdc.noaa.gov/eog/data/web_data/v4composites/F182010.v4.tar'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnight_image_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'wget'"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "night_image_url = 'https://ngdc.noaa.gov/eog/data/web_data/v4composites/F182010.v4.tar'\n",
    "wget.download(night_image_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download Rwandan DHS and construct cluster-level aggregates\n",
    "\n",
    "- **INPUT**: \n",
    "  - `rwanda_clusters_location.csv`: Coordinates of the centroid of each cluster\n",
    "- **OUTPUT**: \n",
    "  - `rwanda_cluster_avg_asset_2010.csv`: Comma-delimited file indicated average wealth of each cluster \n",
    "\n",
    "[Demographic and Health Surveys (DHS)](http://dhsprogram.com/What-We-Do/Survey-Types/DHS.cfm) are nationally-representative household surveys that provide data for a wide range of monitoring and impact evaluation indicators in the areas of population, health, and nutrition. For this assignment, you will need to download the [2010 Rwandan DHS data](http://dhsprogram.com/what-we-do/survey/survey-display-364.cfm). **This requires registration, so start early!** Do not forget to request for the GPS dataset. Make sure you understand the structure of the data before starting.\n",
    "\n",
    "Your immediate goal is to take the raw survey data, covering 12,540 households, and compute the average household wealth for each survey cluster (think of a cluster as a village). Refer to the file `Recode6_DHS_22March2013_DHSG4.pdf` for information on these data.\n",
    "\n",
    "Save your output as `rwanda_cluster_avg_asset_2010.csv` and check that it matches the file that we have provided. You will use this file as input to the next step in the assignment.\n",
    "\n",
    "Hints:\n",
    "- `Household Recode` contains all the attributes of each household. It provides datasets with different formats. Feel free to explore the data. You can use `RWHR61FL.DAT` file in Flat ASCII data (.dat) format.\n",
    "- `RWHR61FL.DCF` describes the attributes and the location of each attribute.\n",
    "- Geographic Datasets: `rwge61fl.zip` contains the location of each cluster in Rwanda. It is in the format of shapefile, which needs QGIS or other GIS softwares to open. For those who are not familiar with GIS tools or who want a shortcut, you can also sue the file `rwanda_clusters_location.csv` provided with the problem set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the cluster locations, overlaid on the nightlights data, are shown in the figure below.\n",
    "<img src=\"figure/map1.png\" alt=\"Map\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "import ipystata  \n",
    "import pandas as pd\n",
    "import savReaderWriter as s\n",
    "\n",
    "# read data from .sav file\n",
    "df= pd.DataFrame(list(s.SavReader('D:/Intensive Data/Problem set 1/rwhr61sv/RWHR61FL.SAV')))\n",
    "\n",
    "df2 = pd.DataFrame(df.iloc[:,125])\n",
    "df2['cluster'] = df.iloc[:,2]\n",
    "df2.columns = ['wealth_index', 'cluster']\n",
    "\n",
    "geo_clusters = pd.read_csv('D:/Intensive Data/Problem set 1/diid17-master/Provided/rwanda_clusters_location.csv')\n",
    "geo_clusters.DHSCLUST = geo_clusters.DHSCLUST.astype(int)\n",
    "hh_geo_clusters = geo_clusters[['X','Y','DHSCLUST']]\n",
    "\n",
    "# merge wealth index and coordinates\n",
    "merged = df2.merge(hh_geo_clusters, left_on='cluster', right_on='DHSCLUST')\n",
    "# merged.drop('cluster',axis=1, inplace=True)\n",
    "\n",
    "cluster_centers = merged.groupby(['DHSCLUST','X','Y'], as_index = False).median()\n",
    "cluster_centers.wealth_index = cluster_centers.wealth_index / 100000\n",
    "cluster_centers.columns = cluster_centers.columns.str.replace('DHSCLUST','id')\n",
    "cluster_centers.columns = cluster_centers.columns.str.replace('wealth_index','wealth')\n",
    "\n",
    "\n",
    "# # make csv file\n",
    "# cluster_centers.to_csv('C:/Users/ntran/Desktop/Intensive Data/Problem set 1/diid17-master/rwanda_cluster_avg_asset_2010.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wealth_index</th>\n",
       "      <th>cluster</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>DHSCLUST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307037.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>148863.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193735.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14170.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7971.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84539.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>260313.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>172366.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>222633.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>119464.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>303990.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>74282.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>135887.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>210890.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>158083.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>126187.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>130537.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>113169.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24178.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>95490.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>127202.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>188557.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>101227.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>273323.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>108752.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>30.055079</td>\n",
       "      <td>-1.940534</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>213632.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.056542</td>\n",
       "      <td>-1.957099</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>250343.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.056542</td>\n",
       "      <td>-1.957099</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>280433.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.056542</td>\n",
       "      <td>-1.957099</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>137642.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.056542</td>\n",
       "      <td>-1.957099</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>209327.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.056542</td>\n",
       "      <td>-1.957099</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12510</th>\n",
       "      <td>-2916.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>30.288211</td>\n",
       "      <td>-2.336048</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12511</th>\n",
       "      <td>-8408.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>30.288211</td>\n",
       "      <td>-2.336048</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12512</th>\n",
       "      <td>-16396.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>30.288211</td>\n",
       "      <td>-2.336048</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12513</th>\n",
       "      <td>49397.0</td>\n",
       "      <td>486.0</td>\n",
       "      <td>30.288211</td>\n",
       "      <td>-2.336048</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12514</th>\n",
       "      <td>170.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12515</th>\n",
       "      <td>-41021.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12516</th>\n",
       "      <td>-2650.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12517</th>\n",
       "      <td>-49243.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12518</th>\n",
       "      <td>51401.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12519</th>\n",
       "      <td>21156.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12520</th>\n",
       "      <td>-56968.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12521</th>\n",
       "      <td>-64998.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12522</th>\n",
       "      <td>-15941.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12523</th>\n",
       "      <td>-33812.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12524</th>\n",
       "      <td>-35070.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12525</th>\n",
       "      <td>58160.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12526</th>\n",
       "      <td>-54981.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12527</th>\n",
       "      <td>-40942.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12528</th>\n",
       "      <td>-63277.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12529</th>\n",
       "      <td>-3865.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12530</th>\n",
       "      <td>-52779.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12531</th>\n",
       "      <td>-67891.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12532</th>\n",
       "      <td>-21921.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12533</th>\n",
       "      <td>-53590.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12534</th>\n",
       "      <td>-56494.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12535</th>\n",
       "      <td>27485.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12536</th>\n",
       "      <td>13410.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12537</th>\n",
       "      <td>39569.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12538</th>\n",
       "      <td>-59525.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12539</th>\n",
       "      <td>-42159.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>30.012104</td>\n",
       "      <td>-2.193395</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12540 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       wealth_index  cluster          X         Y  DHSCLUST\n",
       "0          307037.0    121.0  30.055079 -1.940534       121\n",
       "1          148863.0    121.0  30.055079 -1.940534       121\n",
       "2          193735.0    121.0  30.055079 -1.940534       121\n",
       "3           14170.0    121.0  30.055079 -1.940534       121\n",
       "4           -7971.0    121.0  30.055079 -1.940534       121\n",
       "5           84539.0    121.0  30.055079 -1.940534       121\n",
       "6          260313.0    121.0  30.055079 -1.940534       121\n",
       "7          172366.0    121.0  30.055079 -1.940534       121\n",
       "8          222633.0    121.0  30.055079 -1.940534       121\n",
       "9          119464.0    121.0  30.055079 -1.940534       121\n",
       "10         303990.0    121.0  30.055079 -1.940534       121\n",
       "11          74282.0    121.0  30.055079 -1.940534       121\n",
       "12         135887.0    121.0  30.055079 -1.940534       121\n",
       "13         210890.0    121.0  30.055079 -1.940534       121\n",
       "14         158083.0    121.0  30.055079 -1.940534       121\n",
       "15         126187.0    121.0  30.055079 -1.940534       121\n",
       "16         130537.0    121.0  30.055079 -1.940534       121\n",
       "17         113169.0    121.0  30.055079 -1.940534       121\n",
       "18          24178.0    121.0  30.055079 -1.940534       121\n",
       "19          95490.0    121.0  30.055079 -1.940534       121\n",
       "20         127202.0    121.0  30.055079 -1.940534       121\n",
       "21         188557.0    121.0  30.055079 -1.940534       121\n",
       "22         101227.0    121.0  30.055079 -1.940534       121\n",
       "23         273323.0    121.0  30.055079 -1.940534       121\n",
       "24         108752.0    121.0  30.055079 -1.940534       121\n",
       "25         213632.0     29.0  30.056542 -1.957099        29\n",
       "26         250343.0     29.0  30.056542 -1.957099        29\n",
       "27         280433.0     29.0  30.056542 -1.957099        29\n",
       "28         137642.0     29.0  30.056542 -1.957099        29\n",
       "29         209327.0     29.0  30.056542 -1.957099        29\n",
       "...             ...      ...        ...       ...       ...\n",
       "12510       -2916.0    486.0  30.288211 -2.336048       486\n",
       "12511       -8408.0    486.0  30.288211 -2.336048       486\n",
       "12512      -16396.0    486.0  30.288211 -2.336048       486\n",
       "12513       49397.0    486.0  30.288211 -2.336048       486\n",
       "12514         170.0     19.0  30.012104 -2.193395        19\n",
       "12515      -41021.0     19.0  30.012104 -2.193395        19\n",
       "12516       -2650.0     19.0  30.012104 -2.193395        19\n",
       "12517      -49243.0     19.0  30.012104 -2.193395        19\n",
       "12518       51401.0     19.0  30.012104 -2.193395        19\n",
       "12519       21156.0     19.0  30.012104 -2.193395        19\n",
       "12520      -56968.0     19.0  30.012104 -2.193395        19\n",
       "12521      -64998.0     19.0  30.012104 -2.193395        19\n",
       "12522      -15941.0     19.0  30.012104 -2.193395        19\n",
       "12523      -33812.0     19.0  30.012104 -2.193395        19\n",
       "12524      -35070.0     19.0  30.012104 -2.193395        19\n",
       "12525       58160.0     19.0  30.012104 -2.193395        19\n",
       "12526      -54981.0     19.0  30.012104 -2.193395        19\n",
       "12527      -40942.0     19.0  30.012104 -2.193395        19\n",
       "12528      -63277.0     19.0  30.012104 -2.193395        19\n",
       "12529       -3865.0     19.0  30.012104 -2.193395        19\n",
       "12530      -52779.0     19.0  30.012104 -2.193395        19\n",
       "12531      -67891.0     19.0  30.012104 -2.193395        19\n",
       "12532      -21921.0     19.0  30.012104 -2.193395        19\n",
       "12533      -53590.0     19.0  30.012104 -2.193395        19\n",
       "12534      -56494.0     19.0  30.012104 -2.193395        19\n",
       "12535       27485.0     19.0  30.012104 -2.193395        19\n",
       "12536       13410.0     19.0  30.012104 -2.193395        19\n",
       "12537       39569.0     19.0  30.012104 -2.193395        19\n",
       "12538      -59525.0     19.0  30.012104 -2.193395        19\n",
       "12539      -42159.0     19.0  30.012104 -2.193395        19\n",
       "\n",
       "[12540 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test whether night lights data can predict wealth, as observed in DHS\n",
    "\n",
    "Now that you have \"ground truth\" measures of average cluster wealth, your goal is to understand whether the nightlights data can be used to predict wealth. First, merge the DHS and nightlights data, and then fit a model of wealth on nightlights.\n",
    "\n",
    "## 3.1 Merge nightlights and DHS data at cluster level\n",
    "- **INPUT**: \n",
    " - `F182010.v4d_web.stable_lights.avg_vis.tif`: Nightlights data, from Step 1\n",
    " - `rwanda_cluster_avg_asset_2010.csv`: DHS cluster averages, from Step 2\n",
    "- **OUTPUT**: Merged datasets\n",
    " - `DHS_nightlights.csv`: Merged dataset with 492 rows, and 6 columns (one indicates average cluster wealth, 5 nightlights features)\n",
    " - Scatterplot of nightlights vs. DHS wealth\n",
    "\n",
    "Perform a \"spatial join\" to compute the average nighttime luminosity for each of the DHS clusters. To do this, you should take the average of the luminosity values for the nightlights locations surrounding the cluster centroid.\n",
    "\n",
    "Save your output as `DHS_nightlights.csv` and check that it is the same as the file we have provided.\n",
    "\n",
    "Create a scatterplot showing the relationship between average cluster wealth (y-axis) and average nighttime luminosity (x-axis). Your scatterplot should have one dot for each of the 492 DHS clusters. Report the R^2 of the regression line.\n",
    "\n",
    "Hints:\n",
    " - The resolution of each pixel in the nightlight image is about 1km. Use 10 pixels X 10 pixels to average the luminosity of each cluster.\n",
    " - Start by just taking the **Mean** of the luminosity in the 100 pixels and comparing this to cluster average wealth. If you like, you could also compute other luminosity characteristics of each cluster, such as the **Max**, **Min**, **Standard Deviation** of the 100 pixel values, but this step is not required. Note that the file we provide (`DHS_nightlights.csv`) has these added features.\n",
    " - To read the raw raster (nightlights) files, we recommend using the GDAL library. Use `conda install gdal` to install the GDAL library. We have provided some helper code for this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import os.path\n",
    "from osgeo import gdal, ogr, osr\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "from io import StringIO\n",
    "#import cStringIO\n",
    "gdal.UseExceptions()\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "import urllib\n",
    "\n",
    "# Helper function to read a raster file\n",
    "def read_raster(raster_file):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    read_raster\n",
    "\n",
    "    Given a raster file, get the pixel size, pixel location, and pixel value\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raster_file : string\n",
    "        Path to the raster file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_size : float\n",
    "        Pixel size\n",
    "    top_left_x_coords : numpy.ndarray  shape: (number of columns,)\n",
    "        Longitude of the top-left point in each pixel\n",
    "    top_left_y_coords : numpy.ndarray  shape: (number of rows,)\n",
    "        Latitude of the top-left point in each pixel\n",
    "    centroid_x_coords : numpy.ndarray  shape: (number of columns,)\n",
    "        Longitude of the centroid in each pixel\n",
    "    centroid_y_coords : numpy.ndarray  shape: (number of rows,)\n",
    "        Latitude of the centroid in each pixel\n",
    "    bands_data : numpy.ndarray  shape: (number of rows, number of columns, 1)\n",
    "        Pixel value\n",
    "    \"\"\"\n",
    "    raster_dataset = gdal.Open(raster_file, gdal.GA_ReadOnly)\n",
    "    # get project coordination\n",
    "    proj = raster_dataset.GetProjectionRef()\n",
    "    bands_data = []\n",
    "    # Loop through all raster bands\n",
    "    for b in range(1, raster_dataset.RasterCount + 1):\n",
    "        band = raster_dataset.GetRasterBand(b)\n",
    "        bands_data.append(band.ReadAsArray())\n",
    "        no_data_value = band.GetNoDataValue()\n",
    "    bands_data = np.dstack(bands_data)\n",
    "    rows, cols, n_bands = bands_data.shape\n",
    "\n",
    "    # Get the metadata of the raster\n",
    "    geo_transform = raster_dataset.GetGeoTransform()\n",
    "    (upper_left_x, x_size, x_rotation, upper_left_y, y_rotation, y_size) = geo_transform\n",
    "    \n",
    "    # Get location of each pixel\n",
    "    x_size = 1.0 / int(round(1 / float(x_size)))\n",
    "    y_size = - x_size\n",
    "    y_index = np.arange(bands_data.shape[0])\n",
    "    x_index = np.arange(bands_data.shape[1])\n",
    "    top_left_x_coords = upper_left_x + x_index * x_size\n",
    "    top_left_y_coords = upper_left_y + y_index * y_size\n",
    "    # Add half of the cell size to get the centroid of the cell\n",
    "    centroid_x_coords = top_left_x_coords + (x_size / 2)\n",
    "    centroid_y_coords = top_left_y_coords + (y_size / 2)\n",
    "\n",
    "    return (x_size, top_left_x_coords, top_left_y_coords, centroid_x_coords, centroid_y_coords, bands_data)\n",
    "\n",
    "\n",
    "# Helper function to get the pixel index of the point\n",
    "def get_cell_idx(lon, lat, top_left_x_coords, top_left_y_coords):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    get_cell_idx\n",
    "\n",
    "    Given a point location and all the pixel locations of the raster file,\n",
    "    get the column and row index of the point in the raster\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lon : float\n",
    "        Longitude of the point\n",
    "    lat : float\n",
    "        Latitude of the point\n",
    "    top_left_x_coords : numpy.ndarray  shape: (number of columns,)\n",
    "        Longitude of the top-left point in each pixel\n",
    "    top_left_y_coords : numpy.ndarray  shape: (number of rows,)\n",
    "        Latitude of the top-left point in each pixel\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lon_idx : int\n",
    "        Column index\n",
    "    lat_idx : int\n",
    "        Row index\n",
    "    \"\"\"\n",
    "    lon_idx = np.where(top_left_x_coords < lon)[0][-1]\n",
    "    lat_idx = np.where(top_left_y_coords > lat)[0][-1]\n",
    "    return lon_idx, lat_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this illustrates how you can read the nightlight image\n",
    "raster_file = 'F182010.v4d_web.stable_lights.avg_vis.tif'\n",
    "x_size, top_left_x_coords, top_left_y_coords, centroid_x_coords, centroid_y_coords, bands_data = read_raster(raster_file)\n",
    "# save the result in compressed format - see https://docs.scipy.org/doc/numpy/reference/generated/numpy.savez.html\n",
    "np.savez('nightlight.npz', top_left_x_coords=top_left_x_coords, top_left_y_coords=top_left_y_coords, bands_data=bands_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "def get_pixel (x_idx, y_idx):\n",
    "    num_list = []\n",
    "    for i in range(0,10):\n",
    "        for j in range(0,10):\n",
    "            num_list.append(bands_data[(y_idx-5+i),(x_idx-5+j)])\n",
    "    val_max = np.max(num_list)\n",
    "    val_mean = np.mean(num_list)\n",
    "    val_median = np.median(num_list)\n",
    "    val_min = np.min(num_list)\n",
    "    val_std = np.std(num_list)\n",
    "    return val_max, val_mean, val_median, val_min, val_std\n",
    "\n",
    "def night_light_data(raster_file, cluster_centers_file):\n",
    "    cluster_centers = pd.read_csv(cluster_centers_file)\n",
    "    x_size, top_left_x_coords, top_left_y_coords, centroid_x_coords, centroid_y_coords, bands_data = read_raster(raster_file)\n",
    "    lon = cluster_centers['X']\n",
    "    lat = cluster_centers['Y']\n",
    "    lon_idx = lon\n",
    "    lat_idx = lat\n",
    "\n",
    "    # get index\n",
    "    for i in range (0,len(cluster_centers['X'])):\n",
    "        lon_idx[i], lat_idx[i] = get_cell_idx(lon[i], lat[i], top_left_x_coords, top_left_y_coords)\n",
    "        \n",
    "    max_ = []\n",
    "    mean_ = []\n",
    "    median_ = []\n",
    "    min_ = []\n",
    "    std_ = []\n",
    "    \n",
    "    for center in cluster_centers['id']:\n",
    "        val_max, val_mean, val_median, val_min, val_std = get_pixel(int(lon_idx[center-1]), int(lat_idx[center-1]))\n",
    "        max_.append(val_max)\n",
    "        mean_.append(val_mean)\n",
    "        median_.append(val_median)\n",
    "        min_.append(val_min)\n",
    "        std_.append(val_std)\n",
    "\n",
    "    cluster_centers['max_'] = max_\n",
    "    cluster_centers['mean_'] = mean_\n",
    "    cluster_centers['median_'] = median_\n",
    "    cluster_centers['min_'] = min_\n",
    "    cluster_centers['std_'] = std_\n",
    "    \n",
    "    return cluster_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'X'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\ntran\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2133\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2134\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'X'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-57c4c858526f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcluster_centers_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D:/Intensive Data/Final Project/Cluster Coordinate/Rwanda.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcluster_centers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnight_light_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraster_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_centers_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;31m# cluster_centers.to_csv('D:/Intensive Data/Problem set 1/diid17-master/DHS_nightlights.csv', index = False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0c52784c543a>\u001b[0m in \u001b[0;36mnight_light_data\u001b[0;34m(raster_file, cluster_centers_file)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcluster_centers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_centers_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_left_x_coords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_left_y_coords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroid_x_coords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroid_y_coords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbands_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_raster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraster_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlon_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlon\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ntran\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2057\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ntran\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ntran\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ntran\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3542\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3543\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3544\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3545\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ntran\\Anaconda3\\lib\\site-packages\\pandas\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2134\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2136\u001b[0;31m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4433)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas\\index.c:4279)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13742)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas\\src\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas\\hashtable.c:13696)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'X'"
     ]
    }
   ],
   "source": [
    "# input: raster file path and cluster center file path\n",
    "# output: nightlife table.csv and plot night light\n",
    "\n",
    "raster_file = 'D:/Intensive Data/Final Project/Nightlight Image/F182010.v4d_web.stable_lights.avg_vis.tif'\n",
    "cluster_centers_file = 'D:/Intensive Data/Final Project/Cluster Coordinate/Rwanda.csv'\n",
    "\n",
    "cluster_centers = night_light_data(raster_file, cluster_centers_file)\n",
    "# cluster_centers.to_csv('D:/Intensive Data/Problem set 1/diid17-master/DHS_nightlights.csv', index = False)\n",
    "\n",
    "X = cluster_centers['mean_']\n",
    "Y = cluster_centers['wealth']\n",
    "# make the scatter plot\n",
    "def plot_nightlife(xd, yd, order=1, c='r', alpha=1, Rval=False):\n",
    "    #Calculate best fit\n",
    "    coeffs = np.polyfit(xd, yd, order)\n",
    "\n",
    "    intercept = coeffs[-1]\n",
    "    slope = coeffs[-2]\n",
    "    power = coeffs[0] if order == 2 else 0\n",
    "\n",
    "    minxd = np.min(xd)\n",
    "    maxxd = np.max(xd)\n",
    "\n",
    "    xl = np.array([minxd, maxxd])\n",
    "    yl = power * xl ** 2 + slope * xl + intercept\n",
    "\n",
    "    #Calculate R Squared\n",
    "    p = np.poly1d(coeffs)\n",
    "\n",
    "    ybar = np.sum(yd) / len(yd)\n",
    "    ssreg = np.sum((p(xd) - ybar) ** 2)\n",
    "    sstot = np.sum((yd - ybar) ** 2)\n",
    "    Rsqr = ssreg / sstot\n",
    "    plt.scatter(X, Y)\n",
    "    plt.plot(xl, yl)\n",
    "    plt.xlabel('Rwanda Wealth index vs. Mean Nightlight luminosiy')\n",
    "    plt.ylabel('Wealth Index')\n",
    "    plt.title('Mean Nightlight luminosity')\n",
    "\n",
    "    plt.text(0.8 * maxxd + 0.2 * minxd, 0.8 * np.max(yd) + 0.2 * np.min(yd),\n",
    "                 '$R^2 = %0.3f$' % Rsqr)\n",
    "    return print('R2 value is %.6f' % (Rsqr))\n",
    "\n",
    "    # plot points and fit line\n",
    "plot_nightlife(np.log(X), np.log(Y), order=1, c='r', alpha=1, Rval=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Fit a model of wealth as a function of nightlights\n",
    "- **INPUT**: \n",
    " - `DHS_nightlights.csv`, from Step 3.1\n",
    "- **OUTPUT**: \n",
    " - R^2 of model\n",
    " \n",
    "Above, you fit a regression line to illustrate the relationship between cluster average wealth and corresponding cluster nightlights. Now, use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29) to get a better sense of out of sample accuracy.\n",
    "\n",
    "There are two options for this. The basic way, for those new to machine learning, is to randomly divide your dataset into a training and a test dataset. Randomly select 80% of your clusters and fit a model of cluster-average DHS wealth (your response/dependent variable) on nightlights (your predictor/independent variables). You can use a regression or any other model you prefer. Then, use that model to predict the wealth of the remaining 20% of your data, and compare the predicted values to the actual values, and report the R^2 on these 20%.\n",
    "\n",
    "The preferred way is to use 10-fold cross-validation, where you repeat the above procedure 10 times, so that you have 10 different and non-overlapping test sets. Then, you report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds).\n",
    "\n",
    "Hints:\n",
    " - The scikit learn library has built-in functions for [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html) that make this quite easy.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8cc54172a459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnight_life_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'DHS_nightlights.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmeanR2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnight_life_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnight_life_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The mean R2 value of the 10 fold Linear Regression model is %0.6f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmeanR2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-8cc54172a459>\u001b[0m in \u001b[0;36mnight_life_predict\u001b[0;34m(night_life_file)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'mean_'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnight_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnight_fit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'wealth'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnight_fit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#input: path for DHS Night light csv file\n",
    "#output: the mean R2\n",
    "\n",
    "def night_life_predict(night_life_file):\n",
    "    cluster_centers = pd.read_csv(night_life_file)\n",
    "    features = ['mean_']\n",
    "    night_fit = Ridge(alpha=1.0)\n",
    "    fold = StratifiedKFold(Y, n_folds=5, shuffle=True, random_state=0)\n",
    "    scores = cross_val_score(night_fit, cluster_centers[features], cluster_centers['wealth'], cv=10)\n",
    "    pred = night_fit.predict(cluster_centers[features])\n",
    "    meanR2 = scores.mean()\n",
    "    return meanR2\n",
    "    \n",
    "night_life_file = 'DHS_nightlights.csv'\n",
    "meanR2 = night_life_predict(night_life_file)\n",
    "\n",
    "print(\"The mean R2 value of the 10 fold Linear Regression model is %0.6f\" % (meanR2))\n",
    "\n",
    "#\n",
    "# Your code here\n",
    "# 71%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Download daytime satellite imagery \n",
    "- **INPUT**: \n",
    " - Google Maps API key\n",
    " - `Sector_Boundary_2012.shp`: Rwandan shapefile\n",
    "- **OUTPUT**: \n",
    " - Thousands of satellite images (store in directory `google_image/`)\n",
    "\n",
    "We will use the Google Static Maps API to download satellite images. Refer [Google Static Maps introduction](https://developers.google.com/maps/documentation/static-maps/intro) and [Google Static Maps API Usage Limits](https://developers.google.com/maps/documentation/static-maps/usage-limits). You must apply for an API key before downloading. ** Note that it may take you several days to download the required images, so start early!**\n",
    "\n",
    "Download the images from Google at zoom level 16 (pixel resolution is about 2.5m). Set the image size to be 400 pixels X 400 pixels, so that each image you download will cover 1 square kilometer. In this way, each daytime image you download will correspond to a single pixel from the nighttime imagery from Step 1 above.\n",
    "\n",
    "Hints:\n",
    " - You will need to tell Google the locations for which you wish to download images. One way to do this is to use a [shapefiles](https://en.wikipedia.org/wiki/Shapefile) that specifies the borders of Rwanda. We have provided this shapefile (`Sector_Boundary_2012.shp`) as well as a helper function to read in the shapefile.\n",
    " - The function we provide below does not limit the maximum number of images downloaded per day. Note that if you attempt to  download more than the daily limit, Google will return blank images instead of an error.\n",
    " - You can organize the files however you like. However, for later analysis (Steps 6 and beyond), it may help if you organize these daytime images into 64 folders, with one folder indicating the nightlight intensity of the pixel corresponding to the daytime image. In other words, if you download a daytime image for which the corresponding nighttime pixel has value 32, store that daytime image in a folder labeled '32'. This way, all the satellite images within each folder will have the same nightlight intensity. The file name is columnIndex_rowIndex.jpg, in which row index and column index are the index in the nightlight image (See the diagram below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](figure/data_description.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper function to read a shapefile\n",
    "def get_shp_extent(shp_file):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    get_shp_extent\n",
    "\n",
    "    Given a shapefile, get the extent (boundaries)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    shp_file : string\n",
    "        Path to the shapefile\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    extent : tuple\n",
    "        Boundary location of the shapefile (x_min, x_max, y_min, y_max)\n",
    "    \"\"\"\n",
    "    inDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "    inDataSource = inDriver.Open(inShapefile, 0)\n",
    "    inLayer = inDataSource.GetLayer()\n",
    "    extent = inLayer.GetExtent()\n",
    "    # x_min_shp, x_max_shp, y_min_shp, y_max_shp = extent\n",
    "    return extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print' (<ipython-input-9-55cf491300dc>, line 59)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-55cf491300dc>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    print m\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'\n"
     ]
    }
   ],
   "source": [
    "# Helper functions to download images from Google Maps API\n",
    "\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_exponential_multiplier=1000, wait_exponential_max=3600000)\n",
    "def save_img(url, file_path, file_name):\n",
    "    \"\"\"\n",
    "    Function\n",
    "    --------\n",
    "    save_img\n",
    "\n",
    "    Given a url of the map, save the image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string\n",
    "        URL of the map from Google Map Static API\n",
    "    file_path : string\n",
    "        Folder name of the map\n",
    "    file_name : string\n",
    "        File name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    a = urllib.urlopen(url).read()\n",
    "    b = cStringIO.StringIO(a)\n",
    "    image = ndimage.imread(b, mode='RGB')\n",
    "    # when no image exists, api will return an image with the same color. \n",
    "    # and in the center of the image, it said'Sorry. We have no imagery here'.\n",
    "    # we should drop these images if large area of the image has the same color.\n",
    "    if np.array_equal(image[:,:10,:],image[:,10:20,:]):\n",
    "        pass\n",
    "    else:\n",
    "        misc.imsave(file_path + file_name, image[50:450, :, :])\n",
    "\n",
    "# Now read in the shapefile for Rwanda and extract the edges of the country\n",
    "inShapefile = \"data/shp/Sector_Boundary_2012/Sector_Boundary_2012.shp\"\n",
    "x_min_shp, x_max_shp, y_min_shp, y_max_shp = a\n",
    "\n",
    "left_idx, top_idx = get_cell_idx(x_min_shp, y_max_shp, top_left_x_coords, top_left_y_coords)\n",
    "right_idx, bottom_idx = get_cell_idx(x_max_shp, y_min_shp, top_left_x_coords, top_left_y_coords)\n",
    "\n",
    "key = 'YOUR_GOOGLE_MAP_API_KEY'\n",
    "m = 1\n",
    "for i in xrange(left_idx, right_idx + 1):\n",
    "    for j in xrange(top_idx, bottom_idx + 1):\n",
    "        lon = centroid_x_coords[i]\n",
    "        lat = centroid_y_coords[j]\n",
    "        url = 'https://maps.googleapis.com/maps/api/staticmap?center=' + str(lat) + ',' + \\\n",
    "               str(lon) + '&zoom=16&size=400x500&maptype=satellite&key=' + key\n",
    "        lightness = bands_data[j, i, 0]\n",
    "        file_path = 'google_image/' + str(lightness) + '/'\n",
    "        if not os.path.isdir(file_path):\n",
    "            os.makedirs(file_path)\n",
    "        file_name = str(i) + '_' + str(j) +'.jpg'\n",
    "        save_img(url, file_path, file_name)\n",
    "        if m % 100 == 0:\n",
    "            print m\n",
    "        m += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test whether basic features of daytime imagery can predict wealth\n",
    "In step 3, you tested whether nightlight imagery could predict the wealth of Rwandan villages. You will now test whether daytime imagery can predict village wealth. Start by extracting simple metrics from the daytime imagery; in step 6 you will use more sophsticated methods to engineer these features from the images. **You don't need to do this step if you are able to do step 6.**\n",
    "\n",
    "## 5.1. Extract \"basic\" features from daytime imagery\n",
    "- **INPUT**: \n",
    " - `google_image/...`: Raw images, from Step 4\n",
    "- **OUTPUT**: \n",
    " - `google_image_features_basic.csv`: Image features \n",
    "\n",
    "Convert the raw data from the satellite imagery into a set of features that can be used in a machine learning algorithm. A simple way to do this is to take the raw R/G/B values for each pixel and average them for the image. Thus, if an image has 100 pixels, you will have an average R value, an average G value, and an average B value. Create more features by also computing the min, max, median, and standard deviation of R, G, and B for each image. This process will convert each image into a vector of 15 features.\n",
    "\n",
    "Feel free to be creative if you wish to generate additional features from the imagery -- this is similar to the process described in section 2.3 of the paper's supplementary materials. But don't waste too much time, and don't expect these features to be terribly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4492.1137137413025\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "## Input: Path to google image folder\n",
    "## Output: Generate a dataframe with X index, Y index, the max_R, max_G, etc. of ALL images (~ 50000 images)\n",
    "## The function also extract X and Y index in the file name\n",
    "\n",
    "from PIL import Image\n",
    "import glob, os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "# t['g'].append(7, axis = 0)\n",
    "X_idx = []\n",
    "Y_idx = []\n",
    "max_R = []\n",
    "max_G = []\n",
    "max_B = []\n",
    "min_R = []\n",
    "min_G = []\n",
    "min_B = []\n",
    "mean_R = []\n",
    "mean_G = []\n",
    "mean_B = []\n",
    "median_R = []\n",
    "median_G = []\n",
    "median_B = []\n",
    "std_R = []\n",
    "std_G = []\n",
    "std_B = []\n",
    "\n",
    "images_info = pd.DataFrame({'X_idx':[],'Y_idx':[]})\n",
    "for infile in glob.glob(\"C:/Users/ntran/Desktop/Intensive Data/Problem set 1/diid17-master/google_images_full/*.jpg\"):\n",
    "    base = os.path.basename(infile)\n",
    "    file = os.path.splitext(base)[0]\n",
    "    x,y = (int(x) for x in file.split(\"_\"))\n",
    "    X_idx.append(x)\n",
    "    Y_idx.append(y)\n",
    "    im = cv2.imread(infile)\n",
    "    max_R.append(np.array(im).max(axis=(0,1))[0])\n",
    "    max_G.append(np.array(im).max(axis=(0,1))[1])\n",
    "    max_B.append(np.array(im).max(axis=(0,1))[2])\n",
    "    min_R.append(np.array(im).min(axis=(0,1))[0])\n",
    "    min_G.append(np.array(im).min(axis=(0,1))[1])\n",
    "    min_B.append(np.array(im).min(axis=(0,1))[2])\n",
    "    mean_R.append(np.array(im).mean(axis=(0,1))[0])\n",
    "    mean_G.append(np.array(im).mean(axis=(0,1))[1])\n",
    "    mean_B.append(np.array(im).mean(axis=(0,1))[2])\n",
    "    median_R.append(np.median(np.array(im), axis = (0,1))[0])\n",
    "    median_G.append(np.median(np.array(im), axis = (0,1))[1])\n",
    "    median_B.append(np.median(np.array(im), axis = (0,1))[2])\n",
    "    std_R.append(np.array(im).std(axis=(0,1))[0])\n",
    "    std_G.append(np.array(im).std(axis=(0,1))[1])\n",
    "    std_B.append(np.array(im).std(axis=(0,1))[2])\n",
    "\n",
    "images_info['X_idx'] = X_idx\n",
    "images_info['Y_idx'] = Y_idx\n",
    "images_info['max_R'] = max_R\n",
    "images_info['max_G'] = max_G\n",
    "images_info['max_B'] = max_B\n",
    "images_info['min_R'] = min_R\n",
    "images_info['min_G'] = min_G\n",
    "images_info['min_B'] = min_B\n",
    "images_info['mean_R'] = mean_R\n",
    "images_info['mean_G'] = mean_G\n",
    "images_info['mean_B'] = mean_B\n",
    "images_info['median_R'] = median_R\n",
    "images_info['median_G'] = median_G\n",
    "images_info['median_B'] = median_B\n",
    "images_info['std_R'] = std_R\n",
    "images_info['std_G'] = std_G\n",
    "images_info['std_B'] = std_B\n",
    "\n",
    "images_info.to_csv('C:/Users/ntran/Desktop/Intensive Data/Problem set 1/diid17-master/test.csv', index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Merge daytime images with DHS data\n",
    "\n",
    "- **INPUT**: \n",
    " - `google_image_features_basic.csv`: Satellite imagery features, from Step 5.1\n",
    " - `rwanda_cluster_avg_asset_2010.csv`: DHS cluster averages, from Step 2\n",
    "- **OUTPUT**: Merged datasets\n",
    " - `data/model/DHS_daytime.csv`: Merged dataset with 492 rows, and 16 columns (one indicates average cluster wealth, 15 daytime \n",
    " image features)\n",
    "\n",
    "Now that you have feature vectors for each image, you should merge these with the DHS data indicated average cluster wealth. Follow a similar procedure as you did with 3.1, i.e., determine which image feature vectors are associated with each cluster, and then calculate, for each cluster, the average value of each feature.\n",
    "\n",
    "Save your output as `DHS_daytime.csv` and check that it is roughly the same as the file we have provided. There may be slight differences if you chose to calculate a different set of features than those described in 5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ntran\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "import numpy as np\n",
    "# read output csv file (ground truth with wealth info) from Question 3.1\n",
    "cluster_centers = pd.read_csv('C:/Users/ntran/Desktop/Intensive Data/Problem set 1/diid17-master/DHS_nightlights.csv')\n",
    "day = cluster_centers[['id','X', 'Y', 'wealth']]\n",
    "\n",
    "# read output csv file from 5.1\n",
    "images_info = pd.read_csv('C:/Users/ntran/Desktop/Intensive Data/Problem set 1/diid17-master/test.csv')\n",
    "\n",
    "## input: ground truth (ID, X index, Y index and wealth)\n",
    "### val: features of interest\n",
    "## output: DHS day time file\n",
    "\n",
    "def day_data(day, val):\n",
    "    k = []\n",
    "    for row in day['id']:\n",
    "        x = int(day['X'].iloc[row-1])\n",
    "        y = int(day['Y'].iloc[row-1])\n",
    "        num_list = []\n",
    "\n",
    "        for i in range(0,10):\n",
    "            for j in range(0,10):\n",
    "                t = (images_info[(images_info['X_idx']==(x-5+j)) & (images_info['Y_idx']==(y-5+i))][val]).reset_index(drop=True)        \n",
    "                num_list.append(t[0])\n",
    "            # vm = np.mean(num_list)\n",
    "        k.append(np.mean(num_list))\n",
    "    day[val] = k\n",
    "\n",
    "# extract features of interest (max_R, max_G, etc...)\n",
    "features = ['max_R','max_G','max_B','min_R','min_G','min_B','mean_R','mean_G','mean_B','median_R','median_G','median_B','std_R','std_G', 'std_B']\n",
    "\n",
    "for i in (features):\n",
    "    day_data(day, i)\n",
    "\n",
    "## save the merged data into csv file. This should look similar to DHS_daytime    \n",
    "day.to_csv('C:/Users/ntran/Desktop/Intensive Data/Problem set 1/diid17-master/DHS_daytime.csv', index = False)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Fit a model of wealth as a function of basic daytime features\n",
    "- **INPUT**: \n",
    " - `data/model/DHS_daytime.csv`, from Step 5.2\n",
    "- **OUTPUT**: \n",
    " - R^2 of model\n",
    " \n",
    "As in 3.2, use 10-fold cross-validation to fit a model of cluster-level DHS wealth (your response/dependent variable) as a function of the nightlights data (your predictor/independent variables). Since you have a reasonably large number of predictor variables, you should use a model that incorporates some form of regularization (e.g., ridge regression, lasso regression, or a tree-based method).  Report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean R2 value of the 10 fold Linear Regression model is 0.503117\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## function\n",
    "# input: path of day light file (DHS_daytime.csv)\n",
    "# output: r2 of training model\n",
    "\n",
    "def day_light_predict(day_light_file):\n",
    "    day = pd.read_csv(day_light_file)\n",
    "    features = ['max_R','max_G','max_B','min_R','min_G','min_B','mean_R','mean_G','mean_B','median_R','median_G','median_B','std_R','std_G', 'std_B']\n",
    "    day_fit = Ridge(alpha=1)\n",
    "    scores = cross_val_score(day_fit, day[features], day['wealth'], cv=10)\n",
    "    meanR2 = scores.mean()\n",
    "    return meanR2\n",
    "    \n",
    "day_light_file = 'DHS_daytime.csv'\n",
    "meanR2 = day_light_predict(day_light_file)\n",
    "\n",
    "print(\"The mean R2 value of the 10 fold Linear Regression model is %0.6f\" % (meanR2))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Extract features from daytime imagery using deep learning libraries\n",
    "\n",
    "This is where things get interesting. You will use existing libraries to extract more meaningful features from the daytime imagery, similar to what is shown in Fig. 2 of the paper.\n",
    "\n",
    "## 6.1. Use the keras library to use a basic CNN to extract features of the daytime images \n",
    " \n",
    "- **INPUT**: \n",
    " - `google_image/...`: Raw images, from Step 4\n",
    "- **OUTPUT**: \n",
    " - `google_image_features_cnn.csv`: Image features \n",
    "\n",
    "Begin by using a Convolutional Neural Network that has been pre-trained on ImageNet to extract features from the images. We recommend using the [`Keras` library](https://keras.io/), which provides a very straightforward interface to [TensorFlow](https://www.tensorflow.org/).\n",
    "\n",
    "Hints:\n",
    " - This [short intro](https://github.com/fchollet/deep-learning-models/blob/master/README.md) will help you get started with extracting features from the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#\n",
    "# include top = true => remove only the top layer, but false => remove 3 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2. Test whether these new features of satellite imagery can predict wealth\n",
    "- **INPUT**: \n",
    " - `google_image_features_cnn.csv`: Satellite imagery features, from Step 6.1\n",
    " - `rwanda_cluster_avg_asset_2010.csv`: DHS cluster averages, from Step 2\n",
    "- **OUTPUT**: \n",
    " - `data/model/DHS_daytime.csv`: Merged dataset with 492 rows, and 4097 columns (one indicates average cluster wealth, 4096 CNN-based features)\n",
    " - R^2 of model\n",
    " \n",
    "Calculate the average value of each feature for each of the DHS clusters. As in Step 3.1 and 5.2, you will want to aggregate over images near the cluster centroid by taking the average value for each feature. Create a scatterplot showing the relationship between average cluster wealth (y-axis) and the first principal component of all of your image features (x-axis) - in other words, run PCA on your 4096 image features and plot the first PC on the x-axis. Your scatterplot should have one dot for each of the 492 DHS clusters.\n",
    "\n",
    "Use 10-fold cross-validation to fit a model of cluster-level DHS wealth (your response/dependent variable) as a function of the \"deep\" features (your predictor/independent variables). Use a model that incorporates some form of regularization (e.g., ridge regression, lasso regression, or a tree-based method).  Report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Extra Credit 1: Replicate final model and results of Jean et al (2016)\n",
    "\n",
    "The only thing missing at this point is the \"transfer learning\" step. In other words, instead of using the image features extracted by the CNN directly, we want to retrain the CNN to predict nightlights from daytime imagery, and use those features, which presumably are more appropriate to our final prediction task.\n",
    "\n",
    "## 7.1. Use the nightlights to retrain the CNN and extract features\n",
    "\n",
    "- **INPUT**: \n",
    " - `google_image/...`: Raw images, from Step 4\n",
    "- **OUTPUT**: \n",
    " - `google_image_features_cnn_retrained.csv`: Image features \n",
    "\n",
    "Following the approach used in the paper, first divide your daytime images into three groups, corresponding to images where the corresponding night-lights pixel is dim, medium, or bright. Use these values to define your groups: [0, 3), [3, 35), [35, 64). We have given you the code to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Test whether \"deep\" features of satellite imagery can predict wealth\n",
    "- **INPUT**: \n",
    " - `google_image_cnn/...`: Satellite images from 7.1\n",
    "- **OUTPUT**: \n",
    " - `data/model/DHS_CNN.csv`: Merged dataset with 492 rows, and 4097 columns (one indicates average cluster wealth, 4096 CNN features)\n",
    " - R^2 of model\n",
    "\n",
    "Repeat 6.2, except this time use the features generated from 7.1, i.e., the features that have been constructed after transfer learning. As in 6.2, show a scatterplot of the relationship between average cluster wealth (y-axis) and the first principal component of your image features. Then, report the cross-validated R^2 of your model (i.e., the average R^2 of your 10 test folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Extra Credit 2: Construct a high-resolution map of the distribution of predicted wealth\n",
    "- **INPUT**: \n",
    " - Model, image features (data/model/features_all_predictimage_location.csv)\n",
    "- **OUTPUT**: \n",
    " - Map ('poverty_mapping.tiff')\n",
    " \n",
    "Choose your favorite model from the three daytime-based models that you have trained above: 5.3 (basic daytime features), 6.2 (deep daytime features), or 7.2 (transfer-learned daytime features). Use this model to calculate the predicted wealth of every one of your original images. Create a heatmap showing the distribution of predicted wealth in Rwanda. With any luck, it will look something like this:\n",
    "<img src=\"figure/pmap.png\" alt=\"Map\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Your code here\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
